{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of object_detection.ipynb","provenance":[{"file_id":"1-TNOcPm3Jr3fOJG8rnGT9gh60mHUsvaW","timestamp":1629928896073}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kc8MmgZugZWR"},"source":["# Introduction \n","\n","the goal of this notebook is to train a custom object detection model for cars, overall the following steps are implemented:\n","* First the desired object categories are detected\n","* second whether the objects are colored red or not is estimated \n","\n","the inference result is then saved as a CSV file.\n","\n","#Guide on how to use the notebook\n","\n","using the notebook is very straight forward and the most important thing is to mount your google drive and **create the proper directory according** to the following:\n","\n","after mounting your google drive please creat the following directory structure under '/content/drive/MyDrive' :\n","\n","        objectDetection/\n","               data/\n","                   output/\n","                   rawDataset/\n","                   test/\n","                   train/\n","                   valid/\n","                   customTestDataset/\n","                    \n","               trainedModel/\n","                   coco_eval\n","                   eval/\n","                   savedModles/\n","                   \n","\n","This will be the directory where we are going to have our raw data as well as training validation and test set.\n","\n","Following is the explanation for each directory folder:\n","explaining each directory:\n","\n","* 1-\"output\" is the directory where the final CSV is stored\n","* 2- \"rawDataset\" is where we put the provided dataset (images+json file)\n","* 3- \"test\" is where the test data set is created after the function prepare_data_sets() is executed \n","* 4- \"train\" is where the training data set is created after the function prepare_data_sets() is executed \n","* 5- \"valid\" is where the validation data set is created after the function prepare_data_sets() is executed \n","* 6- traineModel is where the final model is stored after completing the training job\n","it's important to note that if you already have a trained model and don't want to run the training job again\n","copy your model here and comment out the training and evaluation lines in the main\n","* 7- saved models is where you can save your trained models\n","* 8- eval is where evaluation data is saved after running the evaluation job\n","* 9- customTestDataset contains a dataset for further testing that i generated using roboflow\n","\n","Therefore do the following: \n","* upload the dataset into the \"rawDataset\" folder\n","* if you don't want to train the model (it takes a lot of time) copy the model file from the \"savedModles\" folder\n","\n","**note** that the zip file already has all the files outputs and models (because i ran the entire pipeline first then saved the zipfile)\n","\n","that is it. now you can run the code cells in the notebook\n","\n","this is a link to all the project files with a similar directory structure:\n","\n","https://drive.google.com/file/d/1T6ACsLNld-NXYaiL8181qJydvYlu6SPi/view?usp=sharing\n","\n","**note** if you dont want to train the model (already have the trained model) dont execute the \"start training\" , \"tensorboard\" and \"start evaluating the model trained\" code cells"]},{"cell_type":"markdown","metadata":{"id":"GvIzcNr8BT5b"},"source":["##Mounting google drive"]},{"cell_type":"code","metadata":{"id":"NgwTpsZaXu43"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yd6NebZ5hP0A"},"source":["# Install Detectron2 Dependencies"]},{"cell_type":"code","metadata":{"id":"wXisIbT1Zqou"},"source":["# install dependencies: (use cu101 because colab has CUDA 10.1)\n","!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n","!pip install cython pyyaml==5.1\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version\n","# opencv is pre-installed on colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wW8A0IHVZ_MR"},"source":["# install detectron2:\n","!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.5/index.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hkh0B3tqegUo"},"source":["## Importing the dependancies"]},{"cell_type":"code","metadata":{"id":"2rCUZZnbhcyl"},"source":["# You may need to restart your runtime prior to this, to let your installation take effect\n"," \n","# Setup detectron2 logger\n","import detectron2\n","# Setup detectron2 logger\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog\n","from detectron2.data.catalog import DatasetCatalog\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.engine import DefaultTrainer\n","from detectron2.evaluation import COCOEvaluator\n","from detectron2.config import get_cfg\n","from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.utils.visualizer import ColorMode\n","from detectron2.utils.visualizer import Visualizer\n"," \n","# import some common libraries\n","import json\n","import shutil\n","import os\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import random\n","import csv\n","import pathlib\n","import glob\n","import random\n","from google.colab.patches import cv2_imshow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBMUfXGxClb0"},"source":["# Data preprocessing and preparation\n","\n","this block will take the images and the associated Json file (in coco format) and creats three sets of data. a set for training a set for validation and a set for testing.\n","\n","**Note** that the datasets will be stored in the test, train, and valid folders "]},{"cell_type":"code","metadata":{"id":"RW7vhtINtmMw"},"source":["cfg = get_cfg()\n"," \n","mainPath =  '/content/drive/MyDrive/objectDetection/'\n","filedir = os.path.join(mainPath,'data','rawDataSet','_annotations.coco.json' );\n","directory = os.path.join(mainPath,'data','rawDataSet')\n"," \n","imList=[];"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzP9IWbZEoO6"},"source":["   \n","file = open(filedir)\n"," \n","#annotation data that is provided \n","annData = json.load(file);\n","                  \n","annListFlt= annData[\"annotations\"]      \n","annDataflt = {\"annotations\": annListFlt}\n","\n","##### dividing to validation test and train \n"," \n","length = len(annListFlt)\n"," \n","# determining the traing validation and test percentages\n","pt=0.7\n","pv=0.2\n"," \n","tre = int(length*pt); #end of train\n","vs = tre; #start of vallidation\n","ve = vs+ int(length*pv);# end of validation\n","ts= ve; #start of test\n","te = length; #end of test \n"," \n","imageTrain=[]\n","imageValid=[]\n","imageTest=[]\n"," \n","trainSet=annData[\"images\"][0:tre];\n","validSet=annData[\"images\"][vs:ve];\n","testSet=annData[\"images\"][ts:te];\n","\n","trainSetann=annData[\"annotations\"][0:tre];\n","validSetann=annData[\"annotations\"][vs:ve];\n","testSetann=annData[\"annotations\"][ts:te];\n"," \n","###copying test files to test folder\n","path=[];\n","for diction in testSet:\n","    path.append(os.path.join(directory,str(diction[\"file_name\"])))\n","for f in path:\n","    shutil.copy(f, os.path.join(mainPath,'data','test'))\n"," \n","################copying the files for training\n","trainPath=[];\n","for diction in trainSet:\n","    trainPath.append(os.path.join(directory,str(diction[\"file_name\"])))\n","for f in trainPath:\n","    shutil.copy(f, os.path.join(mainPath,'data','train'))\n"," \n","###############copying the files for validation\n","validPath=[];\n","for diction in validSet:\n","    validPath.append(os.path.join(directory,str(diction[\"file_name\"])))\n","for f in validPath:\n","    shutil.copy(f, os.path.join(mainPath,'data','valid')) \n"," \n","#bulding the json files for each set \n","CocoFileTrain = {\"categories\": annData[\"categories\"] , \"images\":trainSet ,\"annotations\": trainSetann };\n","CocoFileValid = {\"categories\": annData[\"categories\"] , \"images\":validSet ,\"annotations\": validSetann };\n","CocoFileTest = {\"categories\": annData[\"categories\"] , \"images\":testSet ,\"annotations\": testSetann };\n"," \n"," \n","#########saving the json files\n","with open(os.path.join(mainPath,'data','train','CocoAnnTrain.json'), 'w') as fp1:\n","    json.dump(CocoFileTrain, fp1)\n","with open(os.path.join(mainPath,'data','valid','CocoAnnValid.json'), 'w') as fp2:\n","    json.dump(CocoFileValid, fp2)\n","with open(os.path.join(mainPath,'data','test','CocoAnnTest.json'), 'w') as fp3:\n","    json.dump(CocoFileTest, fp3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1F2rxcIHS0N"},"source":["#Registering the datasets\n","\n","here we will register the generated data sets in the previous step"]},{"cell_type":"code","metadata":{"id":"7Mi9gsZzhokl"},"source":["register_coco_instances(\"my_dataset_train\", {}, os.path.join(mainPath,'data','train','CocoAnnTrain.json'), os.path.join(mainPath,'data','train'))\n","register_coco_instances(\"my_dataset_val\", {}, os.path.join(mainPath,'data','valid','CocoAnnValid.json'), os.path.join(mainPath,'data','valid'))\n","register_coco_instances(\"my_dataset_test\", {}, os.path.join(mainPath,'data','test','CocoAnnTest.json'), os.path.join(mainPath,'data','test'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lUwuLKVxdYx7"},"source":["#Training dataset visualization"]},{"cell_type":"code","metadata":{"id":"BdTAusKE9zUQ"},"source":["#visualize training data\n","my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n","dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n"," \n","import random\n","from detectron2.utils.visualizer import Visualizer\n"," \n","for d in random.sample(dataset_dicts, 3):\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n","    vis = visualizer.draw_dataset_dict(d)\n","    cv2_imshow(vis.get_image()[:, :, ::-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okQbhIYIh_CL"},"source":["# Train Custom Detectron2 Detector"]},{"cell_type":"code","metadata":{"id":"B4UESNQ4tyVm"},"source":["#We are importing our own Trainer Module here to use the COCO validation evaluation during training. Otherwise no validation eval occurs.\n"," \n","from detectron2.engine import DefaultTrainer\n","from detectron2.evaluation import COCOEvaluator\n"," \n","class CocoTrainer(DefaultTrainer):\n"," \n","  @classmethod\n","  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n"," \n","    if output_folder is None:\n","        \n","        output_folder = os.path.join(mainPath,'trainedModel','coco_eval')\n"," \n","    return COCOEvaluator(dataset_name, cfg, False, output_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qlex1CibLbBt"},"source":["##Configuring model for training \n","here we configure our custom object detection model \n","we chose the FASTER RNN model from the model zoo.\n","the most important part here is the line **cfg.MODEL.DEVICE='cpu'** where we configure whether we want CPU computation for training or GPU if the goal is GPU the line must be commented out\n","also for GPU the **cfg.SOLVER.IMS_PER_BATCH**  is very important especially when you get the runtime error. the lower the less memory the GPU uses"]},{"cell_type":"code","metadata":{"id":"TPc8yVBVh52F"},"source":["cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n","cfg.DATASETS.TEST = (\"my_dataset_val\",)\n"," \n","cfg.DATALOADER.NUM_WORKERS = 4\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 1\n","cfg.SOLVER.BASE_LR = 0.001\n"," \n"," \n","cfg.SOLVER.WARMUP_ITERS = 1000\n","cfg.SOLVER.MAX_ITER = 1400 #adjust up if val mAP is still rising, adjust down if overfit\n","cfg.SOLVER.STEPS = (1000, 1500)\n","cfg.SOLVER.GAMMA = 0.05\n"," \n","cfg.MODEL.DEVICE='cpu'\n"," \n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2 #your number of classes + 1\n"," \n","cfg.TEST.EVAL_PERIOD = 500\n"," \n","cfg.OUTPUT_DIR = os.path.join(mainPath,\"trainedModel\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Arkd5_gwLuZX"},"source":["##Start training "]},{"cell_type":"code","metadata":{"id":"jf2rF10UL0P_"},"source":["trainer = CocoTrainer(cfg)\n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IUKLFXB_dnJ4"},"source":["## Tensorboard"]},{"cell_type":"code","metadata":{"id":"oca9rEQKif1h"},"source":["# Look at training curves in tensorboard:\n","%load_ext tensorboard\n","%tensorboard --logdir output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2jsQDRTL6rp"},"source":["##Start evaluating the model trained\n","This function runs an inference on our test set and outputs a table of average Precisions AP which would give us an idea on how the model is performing"]},{"cell_type":"code","metadata":{"id":"HBUdNVhn1rHh"},"source":["#test evaluation\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\n","predictor = DefaultPredictor(cfg)\n","evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=os.path.join(mainPath,\"trainedModel\",\"eval\"))\n","val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n","inference_on_dataset(trainer.model, val_loader, evaluator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3BX34iXw1gU"},"source":["# Inference with Detectron2 Saved Weights (saved model)\n","in this cell, we run inference on the trained model that we have stored in the directory. this cell is independent of the training and when we have a trained model we can use it to do inference on the test set. this cell also outputs three important data structures that are essential to the next part of the algorithm which is detecting the color red"]},{"cell_type":"code","metadata":{"id":"zVBjf0DE7HEW"},"source":["cfg.MODEL.WEIGHTS = os.path.join(mainPath,\"trainedModel\",\"model_final.pth\")\n","cfg.DATASETS.TEST = (\"my_dataset_test\", )\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n","predictor = DefaultPredictor(cfg)\n","test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n"," \n","imCoordinates = [];\n","boxPerIm = {}\n","imname = []\n"," \n","for imageName in glob.glob(os.path.join(mainPath,'data','test','*jpg')):\n","  im = cv2.imread(imageName)\n","  outputs = predictor(im)\n","  \n","  #saving arrays of box coordinates for each image in a list\n","  instance= outputs[\"instances\"]._fields\n","  boxes= instance[\"pred_boxes\"]\n","  boxestensor = boxes.tensor\n","  boxesArray = boxestensor.cpu().detach().numpy()\n","  imCoordinates.append(boxesArray)\n","  #########\n","  \n","  ###puting images and their coordinates in a dictionary \n","  #boxPerIm[imageName].append(boxesArray)\n","  boxPerIm.setdefault(imageName,boxesArray)\n","  imname.append(imageName)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zDVRBk_NmD7"},"source":["## visualizing the oputput predictions"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"WiJ0Ylc_XAUa"},"source":["for imageName in glob.glob(os.path.join(mainPath,'data','test','*jpg')):\n","  im = cv2.imread(imageName)\n","  outputs = predictor(im)\n","  v = Visualizer(im[:, :, ::-1],\n","                metadata=test_metadata, \n","                scale=0.8\n","                  )\n","  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","  cv2_imshow(out.get_image()[:, :, ::-1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuCVtaF2N-9s"},"source":["# Detecting color red\n","\n","The algorithm:\n","\n","* 1- the function recives the coordinates of the boundary boxes from the run_inference_on_test_set() function \n","* 2- then starts running through the test set directory loading images one by one and cropping the objects out using the boundary box coordinates \n","* 3- then saves the cropped image as RGB and counts the number of pixels that have the following RGB value\n","[x,y,z] \n","where 120<x<255 and y<60 and z<60 and abs(y-z)<30 which in my opinion covers most shades of red\n","* 4- then if these pixels make more than 1 percent of the total pixels (ratio of 0.01) in the cropped image the car is detected \n","as red \n","\n","note that the mentioned parameters above where set experimental and are opinion based and one might find better ones , i would say finding more systematic way for determining these parameters would be beneficial.but over all the results i would say are average in estimating the car color."]},{"cell_type":"code","metadata":{"id":"0YKJOnmFOCyH"},"source":["show_red_car = False ######## if true detected red cars will be displayed\n"," \n","i=0;\n"," \n","nameList =[]\n","coordinateList=[]\n","colorRed=[]\n","maskList=[]\n"," \n","for name in imname:\n","    coordinates = boxPerIm[name]\n","    i=0;\n","    for co in coordinates[:,0]:\n","        if i<coordinates.shape[0]:\n","            Wmin =int(coordinates[i][0])\n","            Hmin =int(coordinates[i][1])\n","            Wmax =int(coordinates[i][2])\n","            Hmax =int(coordinates[i][3])\n","            image = cv2.imread(name)\n","            image = image[Hmin:Hmax, Wmin:Wmax]\n","            size = image.shape\n","            imMask = np.zeros((size[0],size[1])) \n","            imlayer = np.zeros((size[0],size[1])) \n","            \n","            ####################\n","            #showing croping out the bunding box \n","            \n","            # cv2.imshow(\"cropped\", image)\n","            # cv2.waitKey(0)\n","            # cv2.destroyAllWindows()\n","            \n","            ####################\n","            \n","            ####estimating the cars color\n","            c1=0\n","            c2=0\n","            \n","            while c1<size[0]:\n","                while c2<size[1]:\n","                    row=image[c1,c2,:]\n","                    row=row.reshape(-1,1)\n","                    \n","                    if 120<int(row[2]) and row[0]<60 and row[1]<60 and abs(row[1]-row[0])<30:\n","                        imMask[c1,c2] = 255\n","                    c2=c2+1\n","                c2=0;\n","                c1=c1+1\n","                \n","            \n","            nRed = np.count_nonzero(imMask)\n","            ratio = nRed/(size[0]*size[1])\n","            \n","            nameList.append(os.path.basename(name))\n","            coordinateList.append(str([Wmin,Hmin,Wmax,Hmax]))\n","            \n","            if ratio>0.01:\n","                colorRed.append(True)\n","                if show_red_car:\n","                   cv2_imshow(image)    \n","            else:\n","                colorRed.append(False)\n","                        \n","            #stacking to build the rgb mask#####\n","            stacked= np.stack((imlayer,imlayer,imMask), axis=2)\n","            maskList.append(stacked)\n","            # cv2.imshow('',stacked)\n","            # cv2.waitKey(0)\n","            # cv2.destroyAllWindows()\n","            \n","            i=i+1;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAaXLntnZUad"},"source":["#Visualizing the image masks \n","Following visualizes the resulting RGB image after extracting pixels that matched our criterion for color red in the above cell\n"]},{"cell_type":"code","metadata":{"id":"XlVM_yIFaG0x"},"source":["for im in maskList:\n","  outputs = predictor(im)\n","  v = Visualizer(im[:, :, ::-1],\n","                metadata=test_metadata, \n","                scale=0.8\n","                  )\n","  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","  cv2_imshow(out.get_image()[:, :, ::-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sFZLzGsXOauZ"},"source":["#CSV output"]},{"cell_type":"code","metadata":{"id":"78xrTu_vOinF"},"source":["finalOut={\"file name\" : nameList , \"box coordinates\": coordinateList , \"is red\": colorRed}; #the final csv out put \n","finalOutdf = pd.DataFrame.from_dict(finalOut)\n","finalOutCSV = finalOutdf.to_csv(index=False)\n","finalOutdf.to_csv(os.path.join(mainPath,'data','output','finalOutCSV.csv'), index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VBQpbSSOz-r"},"source":["#Conclusion \n","\n","concerning the classification performance of the model when it comes to detecting objects the performance seems acceptable\n","\n","concerning the color detection performance, the current algorithm works better with higher quality images i would not call the algorithm accurate.\n","other solutions that may work could be for example considering the average RGB value of the entire image as a metric for detecting color red or training another model on the masks that are generated after applying the initial algorithm.\n"]}]}